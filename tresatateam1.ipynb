{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Load and label your data files\n",
        "# NOTE: You must have uploaded these files to Colab first\n",
        "df_phone = pd.read_csv('phone.csv', names=['value', 'format_type']).dropna()\n",
        "df_phone['label'] = 'Phone Number'\n",
        "\n",
        "df_company = pd.read_csv('company.csv', names=['value']).dropna()\n",
        "df_company['label'] = 'Company Name'\n",
        "\n",
        "df_country = pd.read_csv('countries.txt', names=['value']).dropna()\n",
        "df_country['label'] = 'Country'\n",
        "\n",
        "df_dates = pd.read_csv('dates.csv', names=['value']).dropna()\n",
        "df_dates['label'] = 'Date'\n",
        "\n",
        "# Combine all data into one DataFrame\n",
        "training_data = pd.concat([df_phone[['value', 'label']], df_company, df_country, df_dates], ignore_index=True)\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined Training Data:\")\n",
        "print(training_data.head())\n",
        "\n",
        "# Feature Engineering\n",
        "# Create a robust set of features from the text data\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    return df\n",
        "\n",
        "training_data_features = create_features(training_data.copy())\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces', 'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot']\n",
        "X = training_data_features[feature_cols]\n",
        "y = training_data_features['label']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nFeatures Created:\")\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "id": "I1NgrjzZ7aKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Train the model\n",
        "# The max_iter is increased to ensure convergence on a large dataset\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the trained model and feature list for later use in predict.py\n",
        "joblib.dump(model, 'semantic_model.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols.pkl')\n",
        "\n",
        "print(\"\\nModel and feature list saved as 'semantic_model.pkl' and 'feature_cols.pkl'\")"
      ],
      "metadata": {
        "id": "692ioRu17kJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Load the list of countries\n",
        "df_country_list = pd.read_csv('countries.txt', names=['country_name'])\n",
        "COUNTRY_LIST = {name.lower() for name in df_country_list['country_name']}\n",
        "\n",
        "# Load and label your training data\n",
        "# NOTE: You must have uploaded these files to Colab first\n",
        "df_phone = pd.read_csv('phone.csv', names=['value', 'format_type']).dropna()\n",
        "df_phone['label'] = 'Phone Number'\n",
        "\n",
        "df_company = pd.read_csv('company.csv', names=['value']).dropna()\n",
        "df_company['label'] = 'Company Name'\n",
        "\n",
        "df_country = pd.read_csv('countries.txt', names=['value']).dropna()\n",
        "df_country['label'] = 'Country'\n",
        "\n",
        "df_dates = pd.read_csv('dates.csv', names=['value']).dropna()\n",
        "df_dates['label'] = 'Date'\n",
        "\n",
        "# Combine all data into one DataFrame\n",
        "training_data = pd.concat([df_phone[['value', 'label']], df_company, df_country, df_dates], ignore_index=True)\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Feature Engineering\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    # --- New Feature ---\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRY_LIST)\n",
        "    return df\n",
        "\n",
        "training_data_features = create_features(training_data.copy())\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces', 'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot', 'is_in_country_list']\n",
        "X = training_data_features[feature_cols]\n",
        "y = training_data_features['label']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nFeatures Created (with new 'is_in_country_list' feature):\")\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "id": "U-rqErxJ8G-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the updated model and feature list\n",
        "joblib.dump(model, 'semantic_model.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols.pkl')\n",
        "\n",
        "print(\"\\nUpdated model and feature list saved.\")"
      ],
      "metadata": {
        "id": "lAFxBtvJ8K0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the updated model and feature list\n",
        "joblib.dump(model, 'semantic_model.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols.pkl')\n",
        "\n",
        "print(\"\\nUpdated model and feature list saved.\")"
      ],
      "metadata": {
        "id": "sdIY5DvY8Q5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# ===============================\n",
        "# 1. Load Data and Create Features\n",
        "# ===============================\n",
        "\n",
        "# Your complete country list (as provided)\n",
        "COUNTRIES = {\n",
        "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\",\n",
        "    \"argentina\", \"armenia\", \"aruba\", \"australia\", \"austria\", \"azerbaijan\",\n",
        "    \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\",\n",
        "    \"belize\", \"benin\", \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\",\n",
        "    \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burma\", \"burundi\",\n",
        "    \"cambodia\", \"cameroon\", \"canada\", \"cape verde\", \"central african republic\",\n",
        "    \"chad\", \"chile\", \"china\", \"colombia\", \"comoros\", \"costa rica\", \"cote d'ivoire\",\n",
        "    \"croatia\", \"cuba\", \"curacao\", \"cyprus\", \"czech republic\",\n",
        "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\",\n",
        "    \"dominican republic\", \"east timor\", \"ecuador\", \"egypt\", \"el salvador\",\n",
        "    \"equatorial guinea\", \"eritrea\", \"estonia\", \"ethiopia\", \"fiji\", \"finland\",\n",
        "    \"france\", \"gabon\", \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\",\n",
        "    \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\",\n",
        "    \"holy see\", \"honduras\", \"hong kong\", \"hungary\", \"iceland\", \"india\",\n",
        "    \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\",\n",
        "    \"japan\", \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kosovo\", \"kuwait\",\n",
        "    \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \"libya\",\n",
        "    \"liechtenstein\", \"lithuania\", \"luxembourg\", \"macau\", \"macedonia\",\n",
        "    \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\",\n",
        "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\",\n",
        "    \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \"morocco\", \"mozambique\",\n",
        "    \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"netherlands antilles\",\n",
        "    \"new zealand\", \"nicaragua\", \"niger\", \"nigeria\", \"north korea\", \"norway\",\n",
        "    \"oman\", \"pakistan\", \"palau\", \"palestinian territories\", \"panama\",\n",
        "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\",\n",
        "    \"qatar\", \"republic of the congo\", \"romania\", \"russia\", \"rwanda\",\n",
        "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\",\n",
        "    \"samoa\", \"san marino\", \"sao tome and principe\", \"saudi arabia\", \"senegal\",\n",
        "    \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"sint maarten\",\n",
        "    \"slovakia\", \"slovenia\", \"solomon islands\", \"somalia\", \"south africa\",\n",
        "    \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\",\n",
        "    \"swaziland\", \"sweden\", \"switzerland\", \"syria\", \"taiwan\", \"tajikistan\",\n",
        "    \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\",\n",
        "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\",\n",
        "    \"united arab emirates\", \"united kingdom\", \"united states\", \"uruguay\",\n",
        "    \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \"yemen\", \"zambia\", \"zimbabwe\"\n",
        "}\n",
        "\n",
        "# Load and label your training data\n",
        "df_phone = pd.read_csv('phone.csv', names=['value', 'format_type']).dropna()\n",
        "df_phone['label'] = 'Phone Number'\n",
        "df_company = pd.read_csv('company.csv', names=['value']).dropna()\n",
        "df_company['label'] = 'Company Name'\n",
        "df_country = pd.read_csv('countries.txt', names=['value']).dropna()\n",
        "df_country['label'] = 'Country'\n",
        "df_dates = pd.read_csv('dates.csv', names=['value']).dropna()\n",
        "df_dates['label'] = 'Date'\n",
        "\n",
        "# Combine all data into one DataFrame\n",
        "training_data = pd.concat([df_phone[['value', 'label']], df_company, df_country, df_dates], ignore_index=True)\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Feature Engineering\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "training_data_features = create_features(training_data.copy())\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces', 'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot', 'is_in_country_list']\n",
        "X = training_data_features[feature_cols]\n",
        "y = training_data_features['label']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ===============================\n",
        "# 2. Train and Evaluate the Model\n",
        "# ===============================\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Final Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the updated model and feature list\n",
        "joblib.dump(model, 'semantic_model.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols.pkl')\n",
        "\n",
        "print(\"\\nUpdated model and feature list saved.\")"
      ],
      "metadata": {
        "id": "VnYJFwme9_P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ===============================\n",
        "# Country list (directly encoded)\n",
        "# ===============================\n",
        "COUNTRIES = {\n",
        "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\",\n",
        "    \"argentina\", \"armenia\", \"aruba\", \"australia\", \"austria\", \"azerbaijan\",\n",
        "    \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\",\n",
        "    \"belize\", \"benin\", \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\",\n",
        "    \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burma\", \"burundi\",\n",
        "    \"cambodia\", \"cameroon\", \"canada\", \"cape verde\", \"central african republic\",\n",
        "    \"chad\", \"chile\", \"china\", \"colombia\", \"comoros\", \"costa rica\", \"cote d'ivoire\",\n",
        "    \"croatia\", \"cuba\", \"curacao\", \"cyprus\", \"czech republic\",\n",
        "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\",\n",
        "    \"dominican republic\", \"east timor\", \"ecuador\", \"egypt\", \"el salvador\",\n",
        "    \"equatorial guinea\", \"eritrea\", \"estonia\", \"ethiopia\", \"fiji\", \"finland\",\n",
        "    \"france\", \"gabon\", \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\",\n",
        "    \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\",\n",
        "    \"holy see\", \"honduras\", \"hong kong\", \"hungary\", \"iceland\", \"india\",\n",
        "    \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\",\n",
        "    \"japan\", \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kosovo\", \"kuwait\",\n",
        "    \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \"libya\",\n",
        "    \"liechtenstein\", \"lithuania\", \"luxembourg\", \"macau\", \"macedonia\",\n",
        "    \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\",\n",
        "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\",\n",
        "    \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \"morocco\", \"mozambique\",\n",
        "    \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"netherlands antilles\",\n",
        "    \"new zealand\", \"nicaragua\", \"niger\", \"nigeria\", \"north korea\", \"norway\",\n",
        "    \"oman\", \"pakistan\", \"palau\", \"palestinian territories\", \"panama\",\n",
        "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\",\n",
        "    \"qatar\", \"republic of the congo\", \"romania\", \"russia\", \"rwanda\",\n",
        "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\",\n",
        "    \"samoa\", \"san marino\", \"sao tome and principe\", \"saudi arabia\", \"senegal\",\n",
        "    \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"sint maarten\",\n",
        "    \"slovakia\", \"slovenia\", \"solomon islands\", \"somalia\", \"south africa\",\n",
        "    \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\",\n",
        "    \"swaziland\", \"sweden\", \"switzerland\", \"syria\", \"taiwan\", \"tajikistan\",\n",
        "    \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\",\n",
        "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\",\n",
        "    \"united arab emirates\", \"united kingdom\", \"united states\", \"uruguay\",\n",
        "    \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \"yemen\", \"zambia\", \"zimbabwe\"\n",
        "}\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# Feature creation (updated)\n",
        "# ===============================\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    # New feature to directly check for countries\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "# ===============================\n",
        "# New classification function\n",
        "# ===============================\n",
        "def classify_column_ml(column: pd.Series):\n",
        "    df_to_predict = pd.DataFrame({'value': column.dropna()})\n",
        "    if df_to_predict.empty:\n",
        "        return {\"prediction\": \"Other\"}\n",
        "\n",
        "    df_features = create_features(df_to_predict)\n",
        "\n",
        "    # Correct feature columns must match what the model was trained on\n",
        "    feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces',\n",
        "                    'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot', 'is_in_country_list']\n",
        "    X_predict = df_features[feature_cols]\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(X_predict)\n",
        "\n",
        "    # Determine the most common prediction and its frequency\n",
        "    prediction_counts = pd.Series(predictions).value_counts(normalize=True)\n",
        "    most_common_pred = prediction_counts.idxmax()\n",
        "    confidence = prediction_counts.max()\n",
        "\n",
        "    return {\"prediction\": most_common_pred, \"scores\": {most_common_pred: confidence}}\n",
        "\n",
        "# ===============================\n",
        "# Main Execution Logic (remains the same)\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Performs semantic classification using a trained ML model.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The path to the input CSV file.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--column\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The name of the column to classify.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: The file '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(args.input)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read the file. Details: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if args.column not in df.columns:\n",
        "        print(f\"Error: The column '{args.column}' was not found in the file.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load the trained model and feature list from the previous steps\n",
        "    try:\n",
        "        model = joblib.load('semantic_model.pkl')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Trained model 'semantic_model.pkl' not found. Please run the training steps first.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    result = classify_column_ml(df[args.column])\n",
        "\n",
        "    print(f\"Input File: {args.input}\")\n",
        "    print(f\"Column Name: {args.column}\")\n",
        "    print(\"--- Classification Result (ML) ---\")\n",
        "    print(f\"Prediction: {result['prediction']}\")\n",
        "    print(f\"Scores: {result['scores']}\")"
      ],
      "metadata": {
        "id": "fZDPU_ki8_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example command\n",
        "!python predict.py --input phone.csv --column number"
      ],
      "metadata": {
        "id": "sDbnK3rd9xTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --\"company\""
      ],
      "metadata": {
        "id": "5x8FY8s59GsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input phone.csv --column \"Phone Number\""
      ],
      "metadata": {
        "id": "BeOWFAl19Qp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input phone.csv --column \"number\""
      ],
      "metadata": {
        "id": "K-b2WGxn9TOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# ===============================================\n",
        "# 1. Load Data and Create Features (Country removed)\n",
        "# ===============================================\n",
        "\n",
        "# Your complete country list (used for feature engineering)\n",
        "COUNTRIES = {\n",
        "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\",\n",
        "    \"argentina\", \"armenia\", \"aruba\", \"australia\", \"austria\", \"azerbaijan\",\n",
        "    \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\",\n",
        "    \"belize\", \"benin\", \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\",\n",
        "    \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burma\", \"burundi\",\n",
        "    \"cambodia\", \"cameroon\", \"canada\", \"cape verde\", \"central african republic\",\n",
        "    \"chad\", \"chile\", \"china\", \"colombia\", \"comoros\", \"costa rica\", \"cote d'ivoire\",\n",
        "    \"croatia\", \"cuba\", \"curacao\", \"cyprus\", \"czech republic\",\n",
        "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\",\n",
        "    \"dominican republic\", \"east timor\", \"ecuador\", \"egypt\", \"el salvador\",\n",
        "    \"equatorial guinea\", \"eritrea\", \"estonia\", \"ethiopia\", \"fiji\", \"finland\",\n",
        "    \"france\", \"gabon\", \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\",\n",
        "    \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\",\n",
        "    \"holy see\", \"honduras\", \"hong kong\", \"hungary\", \"iceland\", \"india\",\n",
        "    \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\",\n",
        "    \"japan\", \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kosovo\", \"kuwait\",\n",
        "    \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \"libya\",\n",
        "    \"liechtenstein\", \"lithuania\", \"luxembourg\", \"macau\", \"macedonia\",\n",
        "    \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\",\n",
        "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\",\n",
        "    \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \"morocco\", \"mozambique\",\n",
        "    \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"netherlands antilles\",\n",
        "    \"new zealand\", \"nicaragua\", \"niger\", \"nigeria\", \"north korea\", \"norway\",\n",
        "    \"oman\", \"pakistan\", \"palau\", \"palestinian territories\", \"panama\",\n",
        "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\",\n",
        "    \"qatar\", \"republic of the congo\", \"romania\", \"russia\", \"rwanda\",\n",
        "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\",\n",
        "    \"samoa\", \"san marino\", \"sao tome and principe\", \"saudi arabia\", \"senegal\",\n",
        "    \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"sint maarten\",\n",
        "    \"slovakia\", \"slovenia\", \"solomon islands\", \"somalia\", \"south africa\",\n",
        "    \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\",\n",
        "    \"swaziland\", \"sweden\", \"switzerland\", \"syria\", \"taiwan\", \"tajikistan\",\n",
        "    \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\",\n",
        "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\",\n",
        "    \"united arab emirates\", \"united kingdom\", \"united states\", \"uruguay\",\n",
        "    \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \"yemen\", \"zambia\", \"zimbabwe\"\n",
        "}\n",
        "\n",
        "\n",
        "# Load and label your training data\n",
        "df_phone = pd.read_csv('phone.csv', names=['value', 'format_type']).dropna()\n",
        "df_phone['label'] = 'Phone Number'\n",
        "df_company = pd.read_csv('company.csv', names=['value']).dropna()\n",
        "df_company['label'] = 'Company Name'\n",
        "df_dates = pd.read_csv('dates.csv', names=['value']).dropna()\n",
        "df_dates['label'] = 'Date'\n",
        "\n",
        "# Combine the data, excluding the problematic 'Country' data\n",
        "training_data = pd.concat([df_phone[['value', 'label']], df_company, df_dates], ignore_index=True)\n",
        "training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Feature Engineering\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "training_data_features = create_features(training_data.copy())\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces', 'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot', 'is_in_country_list']\n",
        "X = training_data_features[feature_cols]\n",
        "y = training_data_features['label']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ===============================\n",
        "# 2. Train and Evaluate the Model\n",
        "# ===============================\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "print(\"Final Classification Report (without 'Country' as a class):\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Save the updated model and feature list for your new predict.py\n",
        "joblib.dump(model, 'semantic_model_no_country.pkl')\n",
        "joblib.dump(feature_cols, 'feature_cols_no_country.pkl')\n",
        "\n",
        "print(\"\\nUpdated model and feature list saved.\")"
      ],
      "metadata": {
        "id": "-8tQPr8r-pXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parser.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# Make sure predict.py is in the same directory\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import classify_column_ml, create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory and contains classify_column_ml, create_features, and COUNTRIES.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Country Code Mapping (extendable)\n",
        "# ==================================\n",
        "COUNTRY_CODE_MAP = {\n",
        "    \"91\": \"India\",\n",
        "    \"1\": \"United States\",\n",
        "    \"44\": \"United Kingdom\",\n",
        "    \"49\": \"Germany\",\n",
        "    \"33\": \"France\",\n",
        "    \"39\": \"Italy\",\n",
        "    \"81\": \"Japan\",\n",
        "    \"86\": \"China\",\n",
        "    \"7\": \"Russia\",\n",
        "    \"61\": \"Australia\",\n",
        "    \"55\": \"Brazil\",\n",
        "    \"27\": \"South Africa\",\n",
        "    \"34\": \"Spain\",\n",
        "    \"82\": \"South Korea\",\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "# ==================================\n",
        "# Legal Suffixes from legal.txt\n",
        "# ==================================\n",
        "LEGAL_TERMS_CONTENT = \"\"\"\n",
        "a spol\n",
        "aat\n",
        "aansprakelijkheid\n",
        "ab\n",
        "actien gesellschaft\n",
        "actiengesellschaft\n",
        "actions\n",
        "ad\n",
        "ae\n",
        "ag\n",
        "agreement\n",
        "aj\n",
        "akc spol\n",
        "akciova\n",
        "aktiebolag\n",
        "aktien\n",
        "andelsselskab\n",
        "allmennaksjeselskap\n",
        "anonimi\n",
        "aksjeselskap\n",
        "aktiengesellschaft\n",
        "aktzii\n",
        "amba\n",
        "anonim ortakligi\n",
        "anonim sirketi\n",
        "anpartsselskab\n",
        "ans\n",
        "ansvar\n",
        "ansvarlig\n",
        "ao\n",
        "aps\n",
        "as\n",
        "asa\n",
        "auf\n",
        "av\n",
        "avoin\n",
        "ay\n",
        "begraenset\n",
        "beperkta\n",
        "berhad\n",
        "beschrankter\n",
        "besloten\n",
        "beteti\n",
        "bhd\n",
        "bt\n",
        "bv\n",
        "bvba\n",
        "cb\n",
        "cic\n",
        "cl\n",
        "co\n",
        "commandite\n",
        "commanditaire\n",
        "community\n",
        "commv\n",
        "compagnie\n",
        "company\n",
        "cooperatief ua\n",
        "cooperatieve\n",
        "cooperative\n",
        "corp\n",
        "corporation\n",
        "cpt\n",
        "cuideachta\n",
        "cv\n",
        "da\n",
        "dat\n",
        "dd\n",
        "de\n",
        "dinteret\n",
        "dionicko\n",
        "dno\n",
        "doo\n",
        "dooel\n",
        "drustvo\n",
        "druzhestvo\n",
        "economique\n",
        "ee\n",
        "en\n",
        "de\n",
        "por\n",
        "einkahlutafelag\n",
        "ehf\n",
        "entreprise\n",
        "esv\n",
        "etaireia\n",
        "eurl\n",
        "fdn\n",
        "felelossegu\n",
        "foundation\n",
        "fp\n",
        "free\n",
        "fz\n",
        "fzco\n",
        "fze\n",
        "general partnership\n",
        "gesellschaft\n",
        "gie\n",
        "gmbh\n",
        "gmbh co kg\n",
        "gp\n",
        "groupement\n",
        "gte\n",
        "haftung\n",
        "haftungsbeschrankt\n",
        "handelsbolag\n",
        "hb\n",
        "helseforetak\n",
        "hf\n",
        "inc\n",
        "incorporated\n",
        "interest\n",
        "ipjsc\n",
        "is\n",
        "javno\n",
        "jtd\n",
        "jawna\n",
        "kb\n",
        "kd\n",
        "kda\n",
        "kft\n",
        "kg\n",
        "komanditsabiedriba\n",
        "komandytne\n",
        "kommanditgesellschaft\n",
        "kgaa\n",
        "kht\n",
        "kommandiittiyhtiö\n",
        "kommandit\n",
        "kozhasznu\n",
        "kozos\n",
        "kommanditbolag\n",
        "korlatolt\n",
        "ks\n",
        "kt\n",
        "kv\n",
        "ky\n",
        "lc\n",
        "lda\n",
        "limitada\n",
        "limitata\n",
        "limited\n",
        "limitee\n",
        "limited liability\n",
        "llc\n",
        "lllp\n",
        "llp\n",
        "lp\n",
        "lt\n",
        "ltd\n",
        "ltda\n",
        "ltee\n",
        "lv\n",
        "mb\n",
        "mbh\n",
        "mchj\n",
        "med\n",
        "met\n",
        "mit\n",
        "nl\n",
        "nuf\n",
        "nv\n",
        "nyrt\n",
        "oaj\n",
        "oao\n",
        "od\n",
        "odgovornoscu\n",
        "odgovornost\n",
        "odpowiedzialnoscia\n",
        "oe\n",
        "offene\n",
        "og\n",
        "one person\n",
        "ood\n",
        "ooo\n",
        "opc\n",
        "ogranicena\n",
        "ogranicenom\n",
        "ograniczona\n",
        "osakeyhtio\n",
        "oy\n",
        "oyj\n",
        "pa\n",
        "partnerska\n",
        "pc\n",
        "per\n",
        "phoibli\n",
        "pjs\n",
        "plc\n",
        "pllc\n",
        "plt\n",
        "pp\n",
        "ppa\n",
        "private\n",
        "privee\n",
        "professional\n",
        "proprietary\n",
        "ps\n",
        "pte\n",
        "pto\n",
        "pty\n",
        "pty ltd\n",
        "public\n",
        "public joint stock company\n",
        "pvt\n",
        "qk\n",
        "responsabilite\n",
        "responsabilita\n",
        "rt\n",
        "s de rl\n",
        "s en c\n",
        "sa\n",
        "sae\n",
        "sal\n",
        "saoc\n",
        "saog\n",
        "sapa\n",
        "sar\n",
        "sarl\n",
        "sas\n",
        "sasu\n",
        "sca\n",
        "scpa\n",
        "scra\n",
        "scs\n",
        "sd\n",
        "sdn\n",
        "sdn bhd\n",
        "se\n",
        "secs\n",
        "selskap\n",
        "sendirin\n",
        "ses\n",
        "sf\n",
        "sgp\n",
        "sha\n",
        "sia\n",
        "sicar\n",
        "sicav\n",
        "simplifiee\n",
        "ska\n",
        "sl\n",
        "slp\n",
        "slne\n",
        "smba\n",
        "smcprivate\n",
        "smcpvt\n",
        "smpc\n",
        "snc\n",
        "soccol\n",
        "sociedad anonima operadora\n",
        "societa per azioni\n",
        "societe anonyme\n",
        "sp z oo\n",
        "sp zoo\n",
        "spzoo\n",
        "spa\n",
        "spj\n",
        "spk\n",
        "spol s ro\n",
        "spolecnost\n",
        "spolka zoo\n",
        "spp\n",
        "sprl\n",
        "srl\n",
        "sro\n",
        "ss\n",
        "stjornarvold\n",
        "stg\n",
        "tapui\n",
        "tarsasag\n",
        "teo\n",
        "theoranta\n",
        "tov\n",
        "tovarystvo\n",
        "trgovacko\n",
        "obch spol\n",
        "uab\n",
        "ug\n",
        "ultd\n",
        "unipersonnelle\n",
        "unlimited\n",
        "unltd\n",
        "vallalat\n",
        "vat\n",
        "vennootschap\n",
        "verwaltungsgesellschaft\n",
        "vof\n",
        "vos\n",
        "vzw\n",
        "xk\n",
        "yhtio\n",
        "yoaj\n",
        "zat\n",
        "zone\n",
        "zrt\n",
        "kscp\n",
        "ab publ\n",
        "ab public\n",
        "qpsc\n",
        "company qpsc\n",
        "sjsc\n",
        "co sjsc\n",
        "pjsc\n",
        "pcl\n",
        "public company limited\n",
        "saa\n",
        "sai\n",
        "tas\n",
        "corporation sjsc\n",
        "bsc\n",
        "abp\n",
        "publikt aktiebolag\n",
        "saic\n",
        "sa esp\n",
        "a s\n",
        "co kscp\n",
        "company kscp\n",
        "saf\n",
        "real estate investment trust\n",
        "reit\n",
        "saa\n",
        "pjsc\n",
        "kk\n",
        "kk\n",
        "kabushiki kaisha\n",
        "sociedad anonima bursatil de capital variable\n",
        "sab de cv\n",
        "sab\n",
        "joint stock company\n",
        "jsc\n",
        "company jsc\n",
        "saai\n",
        "sociedad anonima agricola industrial\n",
        "sacifia\n",
        "anonima comercial industrial financiera inmobiliaria y agropecuaria\n",
        "saci\n",
        "sakp\n",
        "as\n",
        "tao\n",
        "bancorp\n",
        "bancorporation\n",
        "fc spa\n",
        "football club spa\n",
        "shpk\n",
        "shk\n",
        "shoqeri\n",
        "pergjegjesi\n",
        "kufizuar\n",
        "aksionere\n",
        "komandite\n",
        "kolektive\n",
        "dege\n",
        "zyre\n",
        "perfaqesimit\n",
        "responsabilidad\n",
        "comandita simple\n",
        "acciones\n",
        "colectiva\n",
        "capital industria\n",
        "estado\n",
        "garantia\n",
        "reciproca\n",
        "simplificada\n",
        "unipersonal\n",
        "soc col\n",
        "scei\n",
        "sgr\n",
        "sau\n",
        "ilp\n",
        "akcionarsko\n",
        "neogranicenom\n",
        "solidarnom\n",
        "komanditno\n",
        "samostalni\n",
        "preduzetnik\n",
        "sociedade\n",
        "simples\n",
        "coletivo\n",
        "cooperativa\n",
        "publica\n",
        "privada\n",
        "publico\n",
        "eirl\n",
        "empresa\n",
        "individual\n",
        "sc\n",
        "\"\"\"\n",
        "LEGAL_TERMS = {t.strip().lower() for t in LEGAL_TERMS_CONTENT.strip().split(\"\\n\")}\n",
        "\n",
        "# ==================================\n",
        "# Helper: Phone Number Parsing\n",
        "# ==================================\n",
        "def parse_phone_number(phone):\n",
        "    phone = str(phone).strip()\n",
        "    country, number = None, None\n",
        "    clean_phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "\n",
        "    # Check for a country code at the beginning\n",
        "    if clean_phone.startswith('+'):\n",
        "        for code, country_name in COUNTRY_CODE_MAP.items():\n",
        "            if clean_phone.startswith('+' + code):\n",
        "                country = country_name\n",
        "                number = clean_phone[1+len(code):]\n",
        "                return country, number\n",
        "\n",
        "    return None, phone\n",
        "\n",
        "# ==================================\n",
        "# Helper: Company Name Parsing\n",
        "# ==================================\n",
        "def parse_company_name(company):\n",
        "    company = str(company).strip()\n",
        "    lower_company = company.lower()\n",
        "\n",
        "    # Sort legal suffixes by length descending to match longer terms first\n",
        "    sorted_legal_terms = sorted(list(LEGAL_TERMS), key=len, reverse=True)\n",
        "\n",
        "    for suffix in sorted_legal_terms:\n",
        "        if lower_company.endswith(f\" {suffix}\"):\n",
        "            idx = lower_company.rfind(f\" {suffix}\")\n",
        "            name = company[:idx].strip()\n",
        "            legal = company[idx:].strip()\n",
        "            return name, legal\n",
        "    return company, \"\"\n",
        "\n",
        "# ==================================\n",
        "# Main Parsing Logic\n",
        "# ==================================\n",
        "def process_file(input_file):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read file. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    parsed_cols = []\n",
        "\n",
        "    # Identify all columns to be parsed\n",
        "    for col in df.columns:\n",
        "        result = classify_column_ml(df[col])\n",
        "        pred = result[\"prediction\"]\n",
        "        conf = list(result[\"scores\"].values())[0] if \"scores\" in result and result[\"scores\"] else 0\n",
        "\n",
        "        # Check if the confidence for a specific prediction is above a threshold\n",
        "        if pred in [\"Phone Number\", \"Company Name\"] and conf > 0.5:\n",
        "            parsed_cols.append((col, pred, conf))\n",
        "\n",
        "    if not parsed_cols:\n",
        "        print(\"No PhoneNumber or CompanyName column detected with sufficient confidence.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    # Sort to find the best candidate if needed\n",
        "    parsed_cols.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(\"Detected columns for parsing:\")\n",
        "    for col, pred, conf in parsed_cols:\n",
        "        print(f\"- Column '{col}': {pred} (confidence={conf:.2f})\")\n",
        "\n",
        "    output_df = pd.DataFrame()\n",
        "\n",
        "    # Process all detected columns, as required by the problem statement\n",
        "    for col, pred, conf in parsed_cols:\n",
        "        if pred == \"Phone Number\":\n",
        "            output_df[\"PhoneNumber\"] = df[col]\n",
        "            parsed = df[col].apply(lambda x: pd.Series(parse_phone_number(x)))\n",
        "            parsed.columns = [\"Country\", \"Number\"]\n",
        "            output_df = pd.concat([output_df, parsed], axis=1)\n",
        "\n",
        "        elif pred == \"Company Name\":\n",
        "            output_df[\"CompanyName\"] = df[col]\n",
        "            parsed = df[col].apply(lambda x: pd.Series(parse_company_name(x)))\n",
        "            parsed.columns = [\"Name\", \"Legal\"]\n",
        "            output_df = pd.concat([output_df, parsed], axis=1)\n",
        "\n",
        "    output_df.to_csv(\"output.csv\", index=False)\n",
        "    print(\"\\nParsing complete. Output written to output.csv\")\n",
        "\n",
        "# ==================================\n",
        "# CLI\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parses PhoneNumber/CompanyName columns into normalized fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to input CSV file\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    process_file(args.input)"
      ],
      "metadata": {
        "id": "uDOrhcEu-9Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python parser.py --input phone.csv"
      ],
      "metadata": {
        "id": "t_41aoHr_opy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ===============================\n",
        "# Country list (directly encoded)\n",
        "# ===============================\n",
        "COUNTRIES = {\n",
        "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\",\n",
        "    \"argentina\", \"armenia\", \"aruba\", \"australia\", \"austria\", \"azerbaijan\",\n",
        "    \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\",\n",
        "    \"belize\", \"benin\", \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\",\n",
        "    \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burma\", \"burundi\",\n",
        "    \"cambodia\", \"cameroon\", \"canada\", \"cape verde\", \"central african republic\",\n",
        "    \"chad\", \"chile\", \"china\", \"colombia\", \"comoros\", \"costa rica\", \"cote d'ivoire\",\n",
        "    \"croatia\", \"cuba\", \"curacao\", \"cyprus\", \"czech republic\",\n",
        "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\",\n",
        "    \"dominican republic\", \"east timor\", \"ecuador\", \"egypt\", \"el salvador\",\n",
        "    \"equatorial guinea\", \"eritrea\", \"estonia\", \"ethiopia\", \"fiji\", \"finland\",\n",
        "    \"france\", \"gabon\", \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\",\n",
        "    \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\",\n",
        "    \"holy see\", \"honduras\", \"hong kong\", \"hungary\", \"iceland\", \"india\",\n",
        "    \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\",\n",
        "    \"japan\", \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kosovo\", \"kuwait\",\n",
        "    \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \"libya\",\n",
        "    \"liechtenstein\", \"lithuania\", \"luxembourg\", \"macau\", \"macedonia\",\n",
        "    \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\",\n",
        "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\",\n",
        "    \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \"morocco\", \"mozambique\",\n",
        "    \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"netherlands antilles\",\n",
        "    \"new zealand\", \"nicaragua\", \"niger\", \"nigeria\", \"north korea\", \"norway\",\n",
        "    \"oman\", \"pakistan\", \"palau\", \"palestinian territories\", \"panama\",\n",
        "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\",\n",
        "    \"qatar\", \"republic of the congo\", \"romania\", \"russia\", \"rwanda\",\n",
        "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\",\n",
        "    \"samoa\", \"san marino\", \"sao tome and principe\", \"saudi arabia\", \"senegal\",\n",
        "    \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"sint maarten\",\n",
        "    \"slovakia\", \"slovenia\", \"solomon islands\", \"somalia\", \"south africa\",\n",
        "    \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\",\n",
        "    \"swaziland\", \"sweden\", \"switzerland\", \"syria\", \"taiwan\", \"tajikistan\",\n",
        "    \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\",\n",
        "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\",\n",
        "    \"united arab emirates\", \"united kingdom\", \"united states\", \"uruguay\",\n",
        "    \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \"yemen\", \"zambia\", \"zimbabwe\"\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# Feature creation (updated)\n",
        "# ===============================\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "# ===============================\n",
        "# New classification function\n",
        "# ===============================\n",
        "def classify_column_ml(column: pd.Series, model, feature_cols):\n",
        "    df_to_predict = pd.DataFrame({'value': column.dropna()})\n",
        "    if df_to_predict.empty:\n",
        "        return {\"prediction\": \"Other\", \"scores\": {\"Other\": 1.0}}\n",
        "\n",
        "    df_features = create_features(df_to_predict)\n",
        "    X_predict = df_features[feature_cols]\n",
        "\n",
        "    predictions = model.predict(X_predict)\n",
        "    prediction_counts = pd.Series(predictions).value_counts(normalize=True)\n",
        "    most_common_pred = prediction_counts.idxmax()\n",
        "    confidence = prediction_counts.max()\n",
        "\n",
        "    return {\"prediction\": most_common_pred, \"scores\": {most_common_pred: confidence}}\n",
        "\n",
        "# ===============================\n",
        "# Main Execution Logic\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Performs semantic classification using a trained ML model.\"\n",
        "    )\n",
        "    parser.add_argument(\"--input\", type=str, required=True, help=\"The path to the input CSV file.\")\n",
        "    parser.add_argument(\"--column\", type=str, required=True, help=\"The name of the column to classify.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: The file '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(args.input)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read the file. Details: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if args.column not in df.columns:\n",
        "        print(f\"Error: The column '{args.column}' was not found in the file.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load the trained model and feature list\n",
        "    try:\n",
        "        model = joblib.load('semantic_model.pkl')\n",
        "        feature_cols = joblib.load('feature_cols.pkl')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Trained model 'semantic_model.pkl' or 'feature_cols.pkl' not found. Please run the training steps first.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    result = classify_column_ml(df[args.column], model, feature_cols)\n",
        "\n",
        "    print(f\"Input File: {args.input}\")\n",
        "    print(f\"Column Name: {args.column}\")\n",
        "    print(\"--- Classification Result (ML) ---\")\n",
        "    print(f\"Prediction: {result['prediction']}\")\n",
        "    print(f\"Scores: {result['scores']}\")"
      ],
      "metadata": {
        "id": "ibKY6-FLAPCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parser.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import classify_column_ml, create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory and contains classify_column_ml, create_features, and COUNTRIES.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Load the trained model and feature list\n",
        "# This must be done at the top of the script\n",
        "# ==================================\n",
        "try:\n",
        "    model = joblib.load('semantic_model.pkl')\n",
        "    feature_cols = joblib.load('feature_cols.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model files 'semantic_model.pkl' or 'feature_cols.pkl' not found.\")\n",
        "    print(\"Please run the training steps first to create the model and feature files.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# ==================================\n",
        "# Country Code Mapping (extendable)\n",
        "# ==================================\n",
        "COUNTRY_CODE_MAP = {\n",
        "    \"91\": \"India\", \"1\": \"United States\", \"44\": \"United Kingdom\", \"49\": \"Germany\", \"33\": \"France\",\n",
        "    \"39\": \"Italy\", \"81\": \"Japan\", \"86\": \"China\", \"7\": \"Russia\", \"61\": \"Australia\",\n",
        "    \"55\": \"Brazil\", \"27\": \"South Africa\", \"34\": \"Spain\", \"82\": \"South Korea\",\n",
        "}\n",
        "\n",
        "# ==================================\n",
        "# Legal Suffixes from legal.txt\n",
        "# ==================================\n",
        "LEGAL_TERMS_CONTENT = \"\"\"\n",
        "a spol, aat, aansprakelijkheid, ab, actien gesellschaft, actiengesellschaft, actions, ad, ae, ag, agreement, aj, akc spol, akciova, aktiebolag, aktien, andelsselskab, allmennaksjeselskap, anonimi, aksjeselskap, aktiengesellschaft, aktzii, amba, anonim ortakligi, anonim sirketi, anpartsselskab, ans, ansvar, ansvarlig, ao, aps, as, asa, auf, av, avoin, ay, begraenset, beperkta, berhad, beschrankter, besloten, beteti, bhd, bt, bv, bvba, cb, cic, cl, co, commandite, commanditaire, community, commv, compagnie, company, cooperatief ua, cooperatieve, cooperative, corp, corporation, cpt, cuideachta, cv, da, dat, dd, de, dinteret, dionicko, dno, doo, dooel, drustvo, druzhestvo, economique, ee, en, de, por, einkahlutafelag, ehf, entreprise, esv, etaireia, eurl, fdn, felelossegu, foundation, fp, free, fz, fzco, fze, general partnership, gesellschaft, gie, gmbh, gmbh co kg, gp, groupement, gte, haftung, haftungsbeschrankt, handelsbolag, hb, helseforetak, hf, inc, incorporated, interest, ipjsc, is, javno, jtd, jawna, kb, kd, kda, kft, kg, komanditsabiedriba, komandytne, kommanditgesellschaft, kgaa, kht, kommandiittiyhtiö, kommandit, kozhasznu, kozos, kommanditbolag, korlatolt, ks, kt, kv, ky, lc, lda, limitada, limitata, limited, limitee, limited liability, llc, lllp, llp, lp, lt, ltd, ltda, ltee, lv, mb, mbh, mchj, med, met, mit, nl, nuf, nv, nyrt, oaj, oao, od, odgovornoscu, odgovornost, odpowiedzialnoscia, oe, offene, og, one person, ood, ooo, opc, ogranicena, ogranicenom, ograniczona, osakeyhtio, oy, oyj, pa, partnerska, pc, per, phoibli, pjs, plc, pllc, plt, pp, ppa, private, privee, professional, proprietary, ps, pte, pto, pty, pty ltd, public, public joint stock company, pvt, qk, responsabilite, responsabilita, rt, s de rl, s en c, sa, sae, sal, saoc, saog, sapa, sar, sarl, sas, sasu, sca, scpa, scra, scs, sd, sdn, sdn bhd, se, secs, selskap, sendirin, ses, sf, sgp, sha, sia, sicar, sicav, simplifiee, ska, sl, slp, slne, smba, smcprivate, smcpvt, smpc, snc, soccol, sociedad anonima operadora, societa per azioni, societe anonyme, sp z oo, sp zoo, spzoo, spa, spj, spk, spol s ro, spolecnost, spolka zoo, spp, sprl, srl, sro, ss, stjornarvold, stg, tapui, tarsasag, teo, theoranta, tov, tovarystvo, trgovacko, obch spol, uab, ug, ultd, unipersonnelle, unlimited, unltd, vallalat, vat, vennootschap, verwaltungsgesellschaft, vof, vos, vzw, xk, yhtio, yoaj, zat, zone, zrt, kscp, ab publ, ab public, qpsc, company qpsc, sjsc, co sjsc, pjsc, pcl, public company limited, saa, sai, tas, corporation sjsc, bsc, abp, publikt aktiebolag, saic, sa esp, a s, co kscp, company kscp, saf, real estate investment trust, reit, saa, pjsc, kk, kk, kabushiki kaisha, sociedad anonima bursatil de capital variable, sab de cv, sab, joint stock company, jsc, company jsc, saai, sociedad anonima agricola industrial, sacifia, anonima comercial industrial financiera inmobiliaria y agropecuaria, saci, sakp, as, tao, bancorp, bancorporation, fc spa, football club spa, shpk, shk, shoqeri, pergjegjesi, kufizuar, aksionere, komandite, kolektive, dege, zyre, perfaqesimit, responsabilidad, comandita simple, acciones, colectiva, capital industria, estado, garantia, reciproca, simplificada, unipersonal, soc col, scei, sgr, sau, ilp, akcionarsko, neogranicenom, solidarnom, komanditno, samostalni, preduzetnik, sociedade, simples, coletivo, cooperativa, publica, privada, publico, eirl, empresa, individual, sc\n",
        "\"\"\"\n",
        "LEGAL_TERMS = {t.strip().lower() for t in LEGAL_TERMS_CONTENT.strip().split(\",\")}\n",
        "\n",
        "# ==================================\n",
        "# Helper: Phone Number Parsing\n",
        "# ==================================\n",
        "def parse_phone_number(phone):\n",
        "    phone = str(phone).strip()\n",
        "    country, number = None, None\n",
        "    clean_phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "\n",
        "    if clean_phone.startswith('+'):\n",
        "        for code, country_name in COUNTRY_CODE_MAP.items():\n",
        "            if clean_phone.startswith('+' + code):\n",
        "                country = country_name\n",
        "                number = clean_phone[1+len(code):]\n",
        "                return country, number\n",
        "    return None, phone\n",
        "\n",
        "# ==================================\n",
        "# Helper: Company Name Parsing\n",
        "# ==================================\n",
        "def parse_company_name(company):\n",
        "    company = str(company).strip()\n",
        "    lower_company = company.lower()\n",
        "\n",
        "    sorted_legal_terms = sorted(list(LEGAL_TERMS), key=len, reverse=True)\n",
        "\n",
        "    for suffix in sorted_legal_terms:\n",
        "        if lower_company.endswith(f\" {suffix}\"):\n",
        "            idx = lower_company.rfind(f\" {suffix}\")\n",
        "            name = company[:idx].strip()\n",
        "            legal = company[idx:].strip()\n",
        "            return name, legal\n",
        "    return company, \"\"\n",
        "\n",
        "# ==================================\n",
        "# Main Parsing Logic\n",
        "# ==================================\n",
        "def process_file(input_file):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read file. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    parsed_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Pass the model and features to the classification function\n",
        "        result = classify_column_ml(df[col], model, feature_cols)\n",
        "        pred = result[\"prediction\"]\n",
        "        conf = result[\"scores\"][pred] if \"scores\" in result and pred in result[\"scores\"] else 0\n",
        "\n",
        "        if pred in [\"Phone Number\", \"Company Name\"] and conf > 0.5:\n",
        "            parsed_cols.append((col, pred, conf))\n",
        "\n",
        "    if not parsed_cols:\n",
        "        print(\"No PhoneNumber or CompanyName column detected with sufficient confidence.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    parsed_cols.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(\"Detected columns for parsing:\")\n",
        "    for col, pred, conf in parsed_cols:\n",
        "        print(f\"- Column '{col}': {pred} (confidence={conf:.2f})\")\n",
        "\n",
        "    output_df = pd.DataFrame()\n",
        "\n",
        "    for col, pred, conf in parsed_cols:\n",
        "        if pred == \"Phone Number\":\n",
        "            output_df[\"PhoneNumber\"] = df[col]\n",
        "            parsed = df[col].apply(lambda x: pd.Series(parse_phone_number(x)))\n",
        "            parsed.columns = [\"Country\", \"Number\"]\n",
        "            output_df = pd.concat([output_df, parsed], axis=1)\n",
        "\n",
        "        elif pred == \"Company Name\":\n",
        "            output_df[\"CompanyName\"] = df[col]\n",
        "            parsed = df[col].apply(lambda x: pd.Series(parse_company_name(x)))\n",
        "            parsed.columns = [\"Name\", \"Legal\"]\n",
        "            output_df = pd.concat([output_df, parsed], axis=1)\n",
        "\n",
        "    output_df.to_csv(\"output.csv\", index=False)\n",
        "    print(\"\\nParsing complete. Output written to output.csv\")\n",
        "\n",
        "# ==================================\n",
        "# CLI\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parses PhoneNumber/CompanyName columns into normalized fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to input CSV file\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    process_file(args.input)"
      ],
      "metadata": {
        "id": "a-cIGFa2AQ9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python parser.py --input phone.csv"
      ],
      "metadata": {
        "id": "AGccU_9JAWBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Load the trained model and feature list\n",
        "# ==================================\n",
        "try:\n",
        "    model = joblib.load('semantic_model.pkl')\n",
        "    feature_cols = joblib.load('feature_cols.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model files 'semantic_model.pkl' or 'feature_cols.pkl' not found.\")\n",
        "    print(\"Please run the training steps first to create the model and feature files.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Helper function to classify a DataFrame\n",
        "# ==================================\n",
        "def classify_data(df, model, feature_cols):\n",
        "    df_features = create_features(df.copy())\n",
        "    X_predict = df_features[feature_cols]\n",
        "    return model.predict(X_predict)\n",
        "\n",
        "# ==================================\n",
        "# Main Evaluation Logic\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Evaluate on phone.csv\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION REPORT FOR PHONE NUMBERS\")\n",
        "    print(\"=\"*50)\n",
        "    try:\n",
        "        df_phone = pd.read_csv('phone.csv', names=['value']).dropna()\n",
        "        df_phone['label'] = 'Phone Number'\n",
        "        predictions_phone = classify_data(df_phone, model, feature_cols)\n",
        "        print(classification_report(df_phone['label'], predictions_phone, zero_division=0))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'phone.csv' not found.\")\n",
        "\n",
        "    # 2. Evaluate on company.csv\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION REPORT FOR COMPANY NAMES\")\n",
        "    print(\"=\"*50)\n",
        "    try:\n",
        "        df_company = pd.read_csv('company.csv', names=['value']).dropna()\n",
        "        df_company['label'] = 'Company Name'\n",
        "        predictions_company = classify_data(df_company, model, feature_cols)\n",
        "        print(classification_report(df_company['label'], predictions_company, zero_division=0))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'company.csv' not found.\")\n",
        "\n",
        "    # 3. Evaluate on dates.csv\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION REPORT FOR DATES\")\n",
        "    print(\"=\"*50)\n",
        "    try:\n",
        "        df_dates = pd.read_csv('dates.csv', names=['value']).dropna()\n",
        "        df_dates['label'] = 'Date'\n",
        "        predictions_dates = classify_data(df_dates, model, feature_cols)\n",
        "        print(classification_report(df_dates['label'], predictions_dates, zero_division=0))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'dates.csv' not found.\")\n",
        "\n",
        "    # 4. Evaluate on countries.txt\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSIFICATION REPORT FOR COUNTRIES\")\n",
        "    print(\"=\"*50)\n",
        "    try:\n",
        "        df_country = pd.read_csv('countries.txt', names=['value']).dropna()\n",
        "        df_country['label'] = 'Country'\n",
        "        predictions_country = classify_data(df_country, model, feature_cols)\n",
        "        print(classification_report(df_country['label'], predictions_country, zero_division=0))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'countries.txt' not found.\")"
      ],
      "metadata": {
        "id": "jbtp1RwrA9jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python parser.py --input phone.csv"
      ],
      "metadata": {
        "id": "_QAAGj-2D8bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parser.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import classify_column_ml, create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory and contains classify_column_ml, create_features, and COUNTRIES.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Load the trained model and feature list\n",
        "# ==================================\n",
        "try:\n",
        "    model = joblib.load('semantic_model.pkl')\n",
        "    feature_cols = joblib.load('feature_cols.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model files 'semantic_model.pkl' or 'feature_cols.pkl' not found.\")\n",
        "    print(\"Please run the training steps first to create the model and feature files.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Country Code Mapping based on `countries.txt`\n",
        "# ==================================\n",
        "COUNTRY_CODE_MAP = {\n",
        "    \"91\": \"India\", \"1\": \"United States\", \"44\": \"United Kingdom\", \"49\": \"Germany\", \"33\": \"France\",\n",
        "    \"39\": \"Italy\", \"81\": \"Japan\", \"86\": \"China\", \"7\": \"Russia\", \"61\": \"Australia\",\n",
        "    \"55\": \"Brazil\", \"27\": \"South Africa\", \"34\": \"Spain\", \"82\": \"South Korea\",\n",
        "}\n",
        "# Fallback to load countries from file if not directly available\n",
        "try:\n",
        "    with open('countries.txt', 'r', encoding='utf-8') as f:\n",
        "        countries_from_file = {line.strip().lower() for line in f if line.strip()}\n",
        "    # Merging with COUNTRIES from predict.py for comprehensive lookup\n",
        "    all_countries = COUNTRIES.union(countries_from_file)\n",
        "except FileNotFoundError:\n",
        "    all_countries = COUNTRIES\n",
        "\n",
        "# ==================================\n",
        "# Legal Suffixes from legal.txt\n",
        "# ==================================\n",
        "LEGAL_TERMS = set()\n",
        "try:\n",
        "    with open('legal.txt', 'r', encoding='utf-8') as f:\n",
        "        LEGAL_TERMS = {line.strip().lower() for line in f if line.strip()}\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'legal.txt' not found. Company Name parsing may be less accurate.\")\n",
        "    # Fallback with some common terms if file is missing\n",
        "    LEGAL_TERMS = {\"ltd\", \"inc\", \"corp\", \"co\", \"gmbh\", \"ag\", \"llc\", \"plc\", \"sa\"}\n",
        "sorted_legal_terms = sorted(list(LEGAL_TERMS), key=len, reverse=True)\n",
        "\n",
        "# ==================================\n",
        "# Helper: Phone Number Parsing\n",
        "# ==================================\n",
        "def parse_phone_number(phone):\n",
        "    phone = str(phone).strip()\n",
        "    country, number = \"\", phone\n",
        "    clean_phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "\n",
        "    # Check for a country code at the beginning\n",
        "    if clean_phone.startswith('+'):\n",
        "        for code, country_name in COUNTRY_CODE_MAP.items():\n",
        "            if clean_phone.startswith('+' + code):\n",
        "                country = country_name\n",
        "                number = clean_phone[1+len(code):]\n",
        "                return country, number\n",
        "\n",
        "    return country, phone\n",
        "\n",
        "# ==================================\n",
        "# Helper: Company Name Parsing\n",
        "# ==================================\n",
        "def parse_company_name(company):\n",
        "    company = str(company).strip()\n",
        "    lower_company = company.lower()\n",
        "\n",
        "    for suffix in sorted_legal_terms:\n",
        "        if lower_company.endswith(f\" {suffix}\"):\n",
        "            idx = lower_company.rfind(f\" {suffix}\")\n",
        "            name = company[:idx].strip()\n",
        "            legal = company[idx:].strip()\n",
        "            return name, legal\n",
        "    return company, \"\"\n",
        "\n",
        "# ==================================\n",
        "# Main Parsing Logic\n",
        "# ==================================\n",
        "def process_file(input_file):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read file. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    parsed_data = []\n",
        "\n",
        "    # Identify and process all columns in the dataframe\n",
        "    for col in df.columns:\n",
        "        # Use the ML model to classify the column\n",
        "        result = classify_column_ml(df[col], model, feature_cols)\n",
        "        pred = result[\"prediction\"]\n",
        "        conf = result[\"scores\"][pred] if \"scores\" in result and pred in result[\"scores\"] else 0\n",
        "\n",
        "        # Only process columns with a prediction of 'Phone Number' or 'Company Name'\n",
        "        if pred == \"Phone Number\" and conf > 0.5:\n",
        "            # Create the DataFrame with the requested columns for Phone Numbers\n",
        "            phone_df = pd.DataFrame(df[col])\n",
        "            phone_df.columns = ['PhoneNumber']\n",
        "            parsed = phone_df['PhoneNumber'].apply(lambda x: pd.Series(parse_phone_number(x)))\n",
        "            parsed.columns = [\"Country\", \"Number\"]\n",
        "            final_df = pd.concat([phone_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "            print(f\"Table for '{col}' (Phone Number) generated.\")\n",
        "            print(final_df.to_markdown(index=False)) # Print a markdown table\n",
        "\n",
        "        elif pred == \"Company Name\" and conf > 0.5:\n",
        "            # Create the DataFrame with the requested columns for Company Names\n",
        "            company_df = pd.DataFrame(df[col])\n",
        "            company_df.columns = ['CompanyName']\n",
        "            parsed = company_df['CompanyName'].apply(lambda x: pd.Series(parse_company_name(x)))\n",
        "            parsed.columns = [\"Name\", \"Legal\"]\n",
        "            final_df = pd.concat([company_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "            print(f\"Table for '{col}' (Company Name) generated.\")\n",
        "            print(final_df.to_markdown(index=False)) # Print a markdown table\n",
        "\n",
        "        else:\n",
        "            print(f\"Ignoring column '{col}' with prediction '{pred}'.\")\n",
        "\n",
        "    if parsed_data:\n",
        "        # Merge all parsed dataframes into a single output file\n",
        "        final_output_df = pd.concat(parsed_data, axis=1)\n",
        "        final_output_df.to_csv(\"output.csv\", index=False)\n",
        "        print(\"\\nParsing complete. Combined output written to output.csv\")\n",
        "    else:\n",
        "        print(\"\\nNo 'Phone Number' or 'Company Name' columns detected with sufficient confidence.\")\n",
        "\n",
        "# ==================================\n",
        "# CLI\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parses PhoneNumber/CompanyName columns into normalized fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to input CSV file\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    process_file(args.input)"
      ],
      "metadata": {
        "id": "8yvh5ekDEEWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python parser.py --input phone.csv"
      ],
      "metadata": {
        "id": "fBcTuEzjEHYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parser.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import classify_column_ml, create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory and contains classify_column_ml, create_features, and COUNTRIES.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Load the trained model and feature list\n",
        "# ==================================\n",
        "try:\n",
        "    model = joblib.load('semantic_model.pkl')\n",
        "    feature_cols = joblib.load('feature_cols.pkl')\n",
        "    # FIX: Load the text_vectorizer as it's required by classify_column_ml\n",
        "    text_vectorizer = joblib.load('text_vectorizer.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model files 'semantic_model.pkl', 'feature_cols.pkl' or 'text_vectorizer.pkl' not found.\")\n",
        "    print(\"Please run the training steps first to create the model and feature files.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Country Code Mapping based on `countries.txt`\n",
        "# ==================================\n",
        "COUNTRY_CODE_MAP = {\n",
        "    \"91\": \"India\", \"1\": \"United States\", \"44\": \"United Kingdom\", \"49\": \"Germany\", \"33\": \"France\",\n",
        "    \"39\": \"Italy\", \"81\": \"Japan\", \"86\": \"China\", \"7\": \"Russia\", \"61\": \"Australia\",\n",
        "    \"55\": \"Brazil\", \"27\": \"South Africa\", \"34\": \"Spain\", \"82\": \"South Korea\",\n",
        "}\n",
        "# Fallback to load countries from file if not directly available\n",
        "try:\n",
        "    with open('countries.txt', 'r', encoding='utf-8') as f:\n",
        "        countries_from_file = {line.strip().lower() for line in f if line.strip()}\n",
        "    # Merging with COUNTRIES from predict.py for comprehensive lookup\n",
        "    all_countries = COUNTRIES.union(countries_from_file)\n",
        "except FileNotFoundError:\n",
        "    all_countries = COUNTRIES\n",
        "\n",
        "# ==================================\n",
        "# Legal Suffixes from legal.txt\n",
        "# ==================================\n",
        "LEGAL_TERMS = set()\n",
        "try:\n",
        "    with open('legal.txt', 'r', encoding='utf-8') as f:\n",
        "        LEGAL_TERMS = {line.strip().lower() for line in f if line.strip()}\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'legal.txt' not found. Company Name parsing may be less accurate.\")\n",
        "    # Fallback with some common terms if file is missing\n",
        "    LEGAL_TERMS = {\"ltd\", \"inc\", \"corp\", \"co\", \"gmbh\", \"ag\", \"llc\", \"plc\", \"sa\"}\n",
        "sorted_legal_terms = sorted(list(LEGAL_TERMS), key=len, reverse=True)\n",
        "\n",
        "# ==================================\n",
        "# Helper: Phone Number Parsing\n",
        "# ==================================\n",
        "def parse_phone_number(phone):\n",
        "    phone = str(phone).strip()\n",
        "    country, number = \"\", phone\n",
        "    clean_phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "\n",
        "    # Check for a country code at the beginning\n",
        "    if clean_phone.startswith('+'):\n",
        "        for code, country_name in COUNTRY_CODE_MAP.items():\n",
        "            if clean_phone.startswith('+' + code):\n",
        "                country = country_name\n",
        "                number = clean_phone[1+len(code):]\n",
        "                return country, number\n",
        "\n",
        "    return country, phone\n",
        "\n",
        "# ==================================\n",
        "# Helper: Company Name Parsing\n",
        "# ==================================\n",
        "def parse_company_name(company):\n",
        "    company = str(company).strip()\n",
        "    lower_company = company.lower()\n",
        "\n",
        "    for suffix in sorted_legal_terms:\n",
        "        if lower_company.endswith(f\" {suffix}\"):\n",
        "            idx = lower_company.rfind(f\" {suffix}\")\n",
        "            name = company[:idx].strip()\n",
        "            legal = company[idx:].strip()\n",
        "            return name, legal\n",
        "    return company, \"\"\n",
        "\n",
        "# ==================================\n",
        "# Main Parsing Logic\n",
        "# ==================================\n",
        "def process_file(input_file):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read file. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    parsed_data = []\n",
        "\n",
        "    # Identify and process all columns in the dataframe\n",
        "    for col in df.columns:\n",
        "        # Use the ML model to classify the column\n",
        "        # FIX: Pass the text_vectorizer as an argument\n",
        "        result = classify_column_ml(df[col], model, feature_cols, text_vectorizer)\n",
        "        pred = result[\"prediction\"]\n",
        "        # FIX: The predict.py function returns 'confidence', not 'scores'.\n",
        "        conf = result[\"confidence\"] if \"confidence\" in result else 0\n",
        "\n",
        "        # Only process columns with a prediction of 'Phone Number' or 'Company Name'\n",
        "        if pred == \"Phone Number\" and conf > 0.5:\n",
        "            # Create the DataFrame with the requested columns for Phone Numbers\n",
        "            phone_df = pd.DataFrame(df[col])\n",
        "            phone_df.columns = ['PhoneNumber']\n",
        "            parsed = phone_df['PhoneNumber'].apply(lambda x: pd.Series(parse_phone_number(x)))\n",
        "            parsed.columns = [\"Country\", \"Number\"]\n",
        "            final_df = pd.concat([phone_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "            print(f\"Table for '{col}' (Phone Number) generated.\")\n",
        "            print(final_df.to_markdown(index=False)) # Print a markdown table\n",
        "\n",
        "        elif pred == \"Company Name\" and conf > 0.5:\n",
        "            # Create the DataFrame with the requested columns for Company Names\n",
        "            company_df = pd.DataFrame(df[col])\n",
        "            company_df.columns = ['CompanyName']\n",
        "            parsed = company_df['CompanyName'].apply(lambda x: pd.Series(parse_company_name(x)))\n",
        "            parsed.columns = [\"Name\", \"Legal\"]\n",
        "            final_df = pd.concat([company_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "            print(f\"Table for '{col}' (Company Name) generated.\")\n",
        "            print(final_df.to_markdown(index=False)) # Print a markdown table\n",
        "\n",
        "        else:\n",
        "            print(f\"Ignoring column '{col}' with prediction '{pred}'.\")\n",
        "\n",
        "    if parsed_data:\n",
        "        # Merge all parsed dataframes into a single output file\n",
        "        final_output_df = pd.concat(parsed_data, axis=1)\n",
        "        final_output_df.to_csv(\"output.csv\", index=False)\n",
        "        print(\"\\nParsing complete. Combined output written to output.csv\")\n",
        "    else:\n",
        "        print(\"\\nNo 'Phone Number' or 'Company Name' columns detected with sufficient confidence.\")\n",
        "\n",
        "# ==================================\n",
        "# CLI\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parses PhoneNumber/CompanyName columns into normalized fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to input CSV file\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    process_file(args.input)"
      ],
      "metadata": {
        "id": "m9ugeMmmEa79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --\"company\""
      ],
      "metadata": {
        "id": "7HL1Zm8xFGdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parser.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ==================================\n",
        "# Import the classification pipeline\n",
        "# ==================================\n",
        "try:\n",
        "    from predict import classify_column_ml, create_features, COUNTRIES\n",
        "except ImportError:\n",
        "    print(\"Error: 'predict.py' not found. Please ensure it is in the same directory and contains classify_column_ml, create_features, and COUNTRIES.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Load the trained model and feature list\n",
        "# ==================================\n",
        "try:\n",
        "    model = joblib.load('semantic_model.pkl')\n",
        "    feature_cols = joblib.load('feature_cols.pkl')\n",
        "    text_vectorizer = joblib.load('text_vectorizer.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model files 'semantic_model.pkl', 'feature_cols.pkl' or 'text_vectorizer.pkl' not found.\")\n",
        "    print(\"Please run the training steps first to create the model and feature files.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ==================================\n",
        "# Country Code Mapping based on `countries.txt`\n",
        "# ==================================\n",
        "COUNTRY_CODE_MAP = {\n",
        "    \"91\": \"India\", \"1\": \"United States\", \"44\": \"United Kingdom\", \"49\": \"Germany\", \"33\": \"France\",\n",
        "    \"39\": \"Italy\", \"81\": \"Japan\", \"86\": \"China\", \"7\": \"Russia\", \"61\": \"Australia\",\n",
        "    \"55\": \"Brazil\", \"27\": \"South Africa\", \"34\": \"Spain\", \"82\": \"South Korea\",\n",
        "}\n",
        "# Fallback to load countries from file if not directly available\n",
        "try:\n",
        "    with open('countries.txt', 'r', encoding='utf-8') as f:\n",
        "        countries_from_file = {line.strip().lower() for line in f if line.strip()}\n",
        "    all_countries = COUNTRIES.union(countries_from_file)\n",
        "except FileNotFoundError:\n",
        "    all_countries = COUNTRIES\n",
        "\n",
        "# ==================================\n",
        "# Legal Suffixes from legal.txt\n",
        "# ==================================\n",
        "LEGAL_TERMS = set()\n",
        "try:\n",
        "    with open('legal.txt', 'r', encoding='utf-8') as f:\n",
        "        LEGAL_TERMS = {line.strip().lower() for line in f if line.strip()}\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: 'legal.txt' not found. Company Name parsing may be less accurate.\")\n",
        "    LEGAL_TERMS = {\"ltd\", \"inc\", \"corp\", \"co\", \"gmbh\", \"ag\", \"llc\", \"plc\", \"sa\"}\n",
        "sorted_legal_terms = sorted(list(LEGAL_TERMS), key=len, reverse=True)\n",
        "\n",
        "# ==================================\n",
        "# Helper: Phone Number Parsing\n",
        "# ==================================\n",
        "def parse_phone_number(phone):\n",
        "    phone = str(phone).strip()\n",
        "    country, number = \"\", phone\n",
        "    clean_phone = re.sub(r\"[^\\d+]\", \"\", phone)\n",
        "\n",
        "    # Check for a country code at the beginning\n",
        "    if clean_phone.startswith('+'):\n",
        "        for code, country_name in COUNTRY_CODE_MAP.items():\n",
        "            if clean_phone.startswith('+' + code):\n",
        "                country = country_name\n",
        "                # Remove the country code to get the number\n",
        "                number = clean_phone[1+len(code):]\n",
        "                return country, number\n",
        "\n",
        "    return country, phone\n",
        "\n",
        "# ==================================\n",
        "# Helper: Company Name Parsing\n",
        "# ==================================\n",
        "def parse_company_name(company):\n",
        "    company = str(company).strip()\n",
        "    lower_company = company.lower()\n",
        "\n",
        "    for suffix in sorted_legal_terms:\n",
        "        if lower_company.endswith(f\" {suffix.lower()}\"):\n",
        "            idx = lower_company.rfind(f\" {suffix.lower()}\")\n",
        "            name = company[:idx].strip()\n",
        "            legal = company[idx:].strip()\n",
        "            return name, legal\n",
        "    return company, \"\"\n",
        "\n",
        "# ==================================\n",
        "# Main Parsing Logic\n",
        "# ==================================\n",
        "def process_file(input_file):\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read file. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    parsed_data = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        result = classify_column_ml(df[col], model, feature_cols, text_vectorizer)\n",
        "        pred = result[\"prediction\"]\n",
        "        conf = result[\"confidence\"]\n",
        "\n",
        "        if pred == \"Phone Number\" and conf > 0.5:\n",
        "            phone_df = df[[col]].rename(columns={col: 'PhoneNumber'})\n",
        "            parsed = phone_df['PhoneNumber'].apply(lambda x: pd.Series(parse_phone_number(x)))\n",
        "            parsed.columns = [\"Country\", \"Number\"]\n",
        "            final_df = pd.concat([phone_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "\n",
        "        elif pred == \"Company Name\" and conf > 0.5:\n",
        "            company_df = df[[col]].rename(columns={col: 'CompanyName'})\n",
        "            parsed = company_df['CompanyName'].apply(lambda x: pd.Series(parse_company_name(x)))\n",
        "            parsed.columns = [\"Name\", \"Legal\"]\n",
        "            final_df = pd.concat([company_df, parsed], axis=1)\n",
        "            parsed_data.append(final_df)\n",
        "\n",
        "    if parsed_data:\n",
        "        final_output_df = pd.concat(parsed_data, axis=1)\n",
        "        final_output_df.to_csv(\"output.csv\", index=False)\n",
        "        print(\"Parsing complete. Output written to output.csv\")\n",
        "    else:\n",
        "        print(\"No valid columns detected for parsing.\")\n",
        "\n",
        "# ==================================\n",
        "# CLI\n",
        "# ==================================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parses data columns into normalized fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to input CSV file\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    process_file(args.input)"
      ],
      "metadata": {
        "id": "ULFOMVTJFKUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "# ===============================\n",
        "# Country list (directly encoded)\n",
        "# ===============================\n",
        "COUNTRIES = {\n",
        "    \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"antigua and barbuda\",\n",
        "    \"argentina\", \"armenia\", \"aruba\", \"australia\", \"austria\", \"azerbaijan\",\n",
        "    \"bahamas\", \"bahrain\", \"bangladesh\", \"barbados\", \"belarus\", \"belgium\",\n",
        "    \"belize\", \"benin\", \"bhutan\", \"bolivia\", \"bosnia and herzegovina\", \"botswana\",\n",
        "    \"brazil\", \"brunei\", \"bulgaria\", \"burkina faso\", \"burma\", \"burundi\",\n",
        "    \"cambodia\", \"cameroon\", \"canada\", \"cape verde\", \"central african republic\",\n",
        "    \"chad\", \"chile\", \"china\", \"colombia\", \"comoros\", \"costa rica\", \"cote d'ivoire\",\n",
        "    \"croatia\", \"cuba\", \"curacao\", \"cyprus\", \"czech republic\",\n",
        "    \"democratic republic of the congo\", \"denmark\", \"djibouti\", \"dominica\",\n",
        "    \"dominican republic\", \"east timor\", \"ecuador\", \"egypt\", \"el salvador\",\n",
        "    \"equatorial guinea\", \"eritrea\", \"estonia\", \"ethiopia\", \"fiji\", \"finland\",\n",
        "    \"france\", \"gabon\", \"gambia\", \"georgia\", \"germany\", \"ghana\", \"greece\",\n",
        "    \"grenada\", \"guatemala\", \"guinea\", \"guinea bissau\", \"guyana\", \"haiti\",\n",
        "    \"holy see\", \"honduras\", \"hong kong\", \"hungary\", \"iceland\", \"india\",\n",
        "    \"indonesia\", \"iran\", \"iraq\", \"ireland\", \"israel\", \"italy\", \"jamaica\",\n",
        "    \"japan\", \"jordan\", \"kazakhstan\", \"kenya\", \"kiribati\", \"kosovo\", \"kuwait\",\n",
        "    \"kyrgyzstan\", \"laos\", \"latvia\", \"lebanon\", \"lesotho\", \"liberia\", \"libya\",\n",
        "    \"liechtenstein\", \"lithuania\", \"luxembourg\", \"macau\", \"macedonia\",\n",
        "    \"madagascar\", \"malawi\", \"malaysia\", \"maldives\", \"mali\", \"malta\",\n",
        "    \"marshall islands\", \"mauritania\", \"mauritius\", \"mexico\", \"micronesia\",\n",
        "    \"moldova\", \"monaco\", \"mongolia\", \"montenegro\", \"morocco\", \"mozambique\",\n",
        "    \"namibia\", \"nauru\", \"nepal\", \"netherlands\", \"netherlands antilles\",\n",
        "    \"new zealand\", \"nicaragua\", \"niger\", \"nigeria\", \"north korea\", \"norway\",\n",
        "    \"oman\", \"pakistan\", \"palau\", \"palestinian territories\", \"panama\",\n",
        "    \"papua new guinea\", \"paraguay\", \"peru\", \"philippines\", \"poland\", \"portugal\",\n",
        "    \"qatar\", \"republic of the congo\", \"romania\", \"russia\", \"rwanda\",\n",
        "    \"saint kitts and nevis\", \"saint lucia\", \"saint vincent and the grenadines\",\n",
        "    \"samoa\", \"san marino\", \"sao tome and principe\", \"saudi arabia\", \"senegal\",\n",
        "    \"serbia\", \"seychelles\", \"sierra leone\", \"singapore\", \"sint maarten\",\n",
        "    \"slovakia\", \"slovenia\", \"solomon islands\", \"somalia\", \"south africa\",\n",
        "    \"south korea\", \"south sudan\", \"spain\", \"sri lanka\", \"sudan\", \"suriname\",\n",
        "    \"swaziland\", \"sweden\", \"switzerland\", \"syria\", \"taiwan\", \"tajikistan\",\n",
        "    \"tanzania\", \"thailand\", \"timor leste\", \"togo\", \"tonga\", \"trinidad and tobago\",\n",
        "    \"tunisia\", \"turkey\", \"turkmenistan\", \"tuvalu\", \"uganda\", \"ukraine\",\n",
        "    \"united arab emirates\", \"united kingdom\", \"united states\", \"uruguay\",\n",
        "    \"uzbekistan\", \"vanuatu\", \"venezuela\", \"vietnam\", \"yemen\", \"zambia\", \"zimbabwe\"\n",
        "}\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# Feature creation (updated)\n",
        "# ===============================\n",
        "def create_features(df):\n",
        "    df['text_length'] = df['value'].apply(lambda x: len(str(x)))\n",
        "    df['num_digits'] = df['value'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
        "    df['num_letters'] = df['value'].apply(lambda x: sum(c.isalpha() for c in str(x)))\n",
        "    df['num_spaces'] = df['value'].apply(lambda x: sum(c.isspace() for c in str(x)))\n",
        "    df['has_plus'] = df['value'].apply(lambda x: '+' in str(x))\n",
        "    df['has_paren'] = df['value'].apply(lambda x: '(' in str(x) or ')' in str(x))\n",
        "    df['has_hyphen'] = df['value'].apply(lambda x: '-' in str(x))\n",
        "    df['has_slash'] = df['value'].apply(lambda x: '/' in str(x))\n",
        "    df['has_dot'] = df['value'].apply(lambda x: '.' in str(x))\n",
        "    # New feature to directly check for countries\n",
        "    df['is_in_country_list'] = df['value'].apply(lambda x: str(x).lower() in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "# ===============================\n",
        "# New classification function\n",
        "# ===============================\n",
        "def classify_column_ml(column: pd.Series):\n",
        "    df_to_predict = pd.DataFrame({'value': column.dropna()})\n",
        "    if df_to_predict.empty:\n",
        "        return {\"prediction\": \"Other\"}\n",
        "\n",
        "    df_features = create_features(df_to_predict)\n",
        "\n",
        "    # Correct feature columns must match what the model was trained on\n",
        "    feature_cols = ['text_length', 'num_digits', 'num_letters', 'num_spaces',\n",
        "                    'has_plus', 'has_paren', 'has_hyphen', 'has_slash', 'has_dot', 'is_in_country_list']\n",
        "    X_predict = df_features[feature_cols]\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(X_predict)\n",
        "\n",
        "    # Determine the most common prediction and its frequency\n",
        "    prediction_counts = pd.Series(predictions).value_counts(normalize=True)\n",
        "    most_common_pred = prediction_counts.idxmax()\n",
        "    confidence = prediction_counts.max()\n",
        "\n",
        "    return {\"prediction\": most_common_pred, \"scores\": {most_common_pred: confidence}}\n",
        "\n",
        "# ===============================\n",
        "# Main Execution Logic (remains the same)\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Performs semantic classification using a trained ML model.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The path to the input CSV file.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--column\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The name of the column to classify.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: The file '{args.input}' does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(args.input)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Failed to read the file. Details: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if args.column not in df.columns:\n",
        "        print(f\"Error: The column '{args.column}' was not found in the file.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load the trained model and feature list from the previous steps\n",
        "    try:\n",
        "        model = joblib.load('semantic_model.pkl')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Trained model 'semantic_model.pkl' not found. Please run the training steps first.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    result = classify_column_ml(df[args.column])\n",
        "\n",
        "    print(f\"Input File: {args.input}\")\n",
        "    print(f\"Column Name: {args.column}\")\n",
        "    print(\"--- Classification Result (ML) ---\")\n",
        "    print(f\"Prediction: {result['prediction']}\")\n",
        "    print(f\"Scores: {result['scores']}\")"
      ],
      "metadata": {
        "id": "4hsa3OeEFT22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --\"company\""
      ],
      "metadata": {
        "id": "q1gJg0rbFVf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --company"
      ],
      "metadata": {
        "id": "98YBplvEFdt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --column \"company\""
      ],
      "metadata": {
        "id": "nifIMFjdFgo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "import pandas as pd\n",
        "import re\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Load data files\n",
        "try:\n",
        "    with open('data/countries.txt', 'r', encoding='utf-8') as f:\n",
        "        COUNTRIES = {line.strip().lower() for line in f if line.strip()}\n",
        "    with open('data/legal.txt', 'r', encoding='utf-8') as f:\n",
        "        LEGAL_TERMS = {line.strip().lower() for line in f if line.strip()}\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: Data files not found. Using default lists.\")\n",
        "    COUNTRIES = {\"india\", \"united states\", \"united kingdom\", \"germany\"}\n",
        "    LEGAL_TERMS = {\"inc\", \"ltd\", \"gmbh\"}\n",
        "\n",
        "# Feature creation function\n",
        "def create_features(df):\n",
        "    df['value'] = df['value'].astype(str).str.strip().str.lower()\n",
        "    df['is_phone_format'] = df['value'].apply(lambda x: bool(re.match(r'^[\\s\\(\\)\\+\\-]*\\d[\\d\\s\\(\\)\\+\\-]{6,20}$', x)))\n",
        "    df['is_company_legal_term'] = df['value'].apply(lambda x: any(term in x for term in LEGAL_TERMS))\n",
        "    df['is_numeric'] = df['value'].apply(lambda x: x.replace('.', '', 1).isdigit())\n",
        "    df['is_country'] = df['value'].apply(lambda x: x in COUNTRIES)\n",
        "    return df\n",
        "\n",
        "# A simple training dataset to make the model runnable\n",
        "data = {\n",
        "    'value': [\n",
        "        '+91 9876543210', '(212) 555-1234', 'The Coca-Cola Company', 'Google Inc.',\n",
        "        '2023-01-15', 'Jan 20, 2024', 'India', 'United Kingdom'\n",
        "    ],\n",
        "    'label': [\n",
        "        'Phone Number', 'Phone Number', 'Company Name', 'Company Name',\n",
        "        'Date', 'Date', 'Country', 'Country'\n",
        "    ]\n",
        "}\n",
        "train_df = pd.DataFrame(data)\n",
        "\n",
        "# Create features and train the model\n",
        "train_df = create_features(train_df)\n",
        "X = train_df.drop('label', axis=1)\n",
        "y = train_df['label']\n",
        "\n",
        "text_features = X['value']\n",
        "text_vectorizer = TfidfVectorizer(max_features=10)\n",
        "text_features_transformed = text_vectorizer.fit_transform(text_features)\n",
        "X_features = X.drop('value', axis=1)\n",
        "feature_cols = X_features.columns.tolist()\n",
        "\n",
        "X_combined = pd.concat([pd.DataFrame(text_features_transformed.toarray()), X_features.reset_index(drop=True)], axis=1)\n",
        "X_combined.columns = [f'tfidf_{i}' for i in range(text_features_transformed.shape[1])] + feature_cols\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "model.fit(X_combined, y)\n",
        "\n",
        "# Save the model and feature columns for later use\n",
        "joblib.dump(model, 'semantic_model.pkl')\n",
        "joblib.dump(X_combined.columns.tolist(), 'feature_cols.pkl')\n",
        "joblib.dump(text_vectorizer, 'text_vectorizer.pkl')\n",
        "\n",
        "def classify_column_ml(column_series, model, feature_cols, text_vectorizer):\n",
        "    df_temp = pd.DataFrame(column_series)\n",
        "    df_temp.columns = ['value']\n",
        "\n",
        "    # Create the same features as during training\n",
        "    df_features = create_features(df_temp)\n",
        "\n",
        "    text_features = df_features['value']\n",
        "    text_features_transformed = text_vectorizer.transform(text_features)\n",
        "    X_features = df_features.drop('value', axis=1)\n",
        "\n",
        "    X_combined = pd.concat([pd.DataFrame(text_features_transformed.toarray(), index=X_features.index), X_features], axis=1)\n",
        "\n",
        "    # FIX: Reindex the DataFrame to match the feature names from training\n",
        "    X_combined_reordered = X_combined.reindex(columns=feature_cols, fill_value=0)\n",
        "\n",
        "    predictions = model.predict(X_combined_reordered)\n",
        "    probabilities = model.predict_proba(X_combined_reordered)\n",
        "\n",
        "    main_prediction = max(set(predictions), key=list(predictions).count)\n",
        "    confidence = probabilities[0][model.classes_.tolist().index(main_prediction)]\n",
        "\n",
        "    return {\"prediction\": main_prediction, \"confidence\": confidence}\n",
        "\n",
        "# Main script logic for command-line execution\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Classify the semantic type of a column using a trained ML model.\")\n",
        "    parser.add_argument('--input', type=str, required=True, help='The path to the CSV file to classify.')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: File '{args.input}' not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        # Load the saved model components\n",
        "        model = joblib.load('semantic_model.pkl')\n",
        "        feature_cols = joblib.load('feature_cols.pkl')\n",
        "        text_vectorizer = joblib.load('text_vectorizer.pkl')\n",
        "\n",
        "        # Read the entire CSV and assume the first column is the one to classify\n",
        "        df = pd.read_csv(args.input, header=None)\n",
        "        column_to_classify = df.iloc[:, 0]\n",
        "\n",
        "        # Call the classification function\n",
        "        result = classify_column_ml(column_to_classify, model, feature_cols, text_vectorizer)\n",
        "        print(f\"The column's semantic type is: {result['prediction']}\")\n",
        "        print(f\"Confidence score: {result['confidence']:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        print(\"Please ensure the training steps have been run and the model files exist.\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "id": "izLVraHFGKFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input company.csv --\"company\""
      ],
      "metadata": {
        "id": "kFK-YdZOGMNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
